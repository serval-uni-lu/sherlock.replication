{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ1. Effectiveness (Range of Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SBFL results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sblf_result_file = \"results/purely_sbfl/ranks.pkl\"\n",
    "sbfl_df = pd.read_pickle(sblf_result_file)\n",
    "sbfl_formulas = sbfl_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dstar (min ~ max, std): 1 ~ 751 (146.25)\n",
      "ochiai (min ~ max, std): 1 ~ 748 (153.91)\n",
      "tarantula (min ~ max, std): 1 ~ 259 (75.78)\n",
      "barinel (min ~ max, std): 1 ~ 243 (74.98)\n"
     ]
    }
   ],
   "source": [
    "for sbfl_formula in sbfl_formulas:\n",
    "    best_ranks = sbfl_df[sbfl_formula].apply(np.min).values\n",
    "    best_ranks = list(set(best_ranks) - set([1570]))\n",
    "    min_rank = np.min(best_ranks)\n",
    "    max_rank = np.max(best_ranks)\n",
    "    #print(best_ranks)\n",
    "    std_v = np.round(np.std(best_ranks), decimals=2)\n",
    "    print (\"{} (min ~ max, std): {} ~ {} ({:.2f})\".format(sbfl_formula, min_rank, max_rank, std_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SBFL scores (suspiciousness) as GP features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_learn2rank import select_mdl\n",
    "\n",
    "sbfl_gp_result_dir = \"results/gp/sbfl\"\n",
    "fold_file = \"data/data_folds.tsv\"\n",
    "\n",
    "data_folds_df = pd.read_csv(fold_file, delimiter='\\t')\n",
    "data_folds = {int(test_fold_idx):commits.split(\",\") for test_fold_idx,commits in data_folds_df.values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mdl_indices(selected_mdls, indices_to_fold, which = 'med'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    indices_of_models = {}\n",
    "    for idx in indices_to_fold:\n",
    "        indices_of_models[idx] = selected_mdls.loc[\n",
    "            (selected_mdls.fold == idx) & (selected_mdls.which == which)].iter.values[0]\n",
    "    \n",
    "    return indices_of_models\n",
    "\n",
    "def get_ranks_of_specific_mdls(result_dir, data_folds, mdl_indices):\n",
    "    \"\"\"\n",
    "    commits -> target flaky commit (the fix commit of flaky test, more exactly)\n",
    "    mdl_indices -> for each fold\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    ranks_per_commit = {}\n",
    "    for fold_idx, commits_in_fold in data_folds.items():\n",
    "        mdl_idx = mdl_indices[fold_idx]\n",
    "        for commit in commits_in_fold:\n",
    "            result_file_pat = os.path.join(result_dir, \"{}.{}.test.result.csv\".format(commit, mdl_idx))\n",
    "            result_files = glob.glob(result_file_pat)\n",
    "            assert len(result_files) == 1, result_file_pat\n",
    "            result_file = result_files[0]\n",
    "            df = pd.read_csv(result_file)\n",
    "            ranks = df.loc[df.isSusFlaky == 1].sp_rank.values\n",
    "            ranks_per_commit[commit[:8]] = ranks\n",
    "            \n",
    "    return ranks_per_commit\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_mdls = select_mdl(None, 'gp', sbfl_gp_result_dir, \"data\", 30, data_folds)\n",
    "med_mdl_indices = get_mdl_indices(selected_mdls, np.arange(10), which = 'med')\n",
    "med_mdl_ranks = get_ranks_of_specific_mdls(sbfl_gp_result_dir, data_folds, med_mdl_indices)\n",
    "med_mdl_best_ranks = [np.min(rs) for rs in med_mdl_ranks.values()]\n",
    "#best_mdl_indices = get_mdl_indices(selected_mdls, np.arange(10), which = 'best')\n",
    "#best_ranks = get_ranks_of_specific_mdls(sbfl_gp_result_dir, data_folds, best_mdl_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For GP with only SBFL scores as it input features, \n",
      "  ranks: 1 ~ 1570 and std of ranks: 274.34\n"
     ]
    }
   ],
   "source": [
    "print (\"For GP with only SBFL scores as it input features, \\n  ranks: {} ~ {} and std of ranks: {:.2f}\"\n",
    "    .format(np.min(med_mdl_best_ranks), np.max(med_mdl_best_ranks), np.round(np.std(med_mdl_best_ranks), decimals=2)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
